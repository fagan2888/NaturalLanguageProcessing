{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) is concerned with the interaction of computers and natural human languages. We can model some NLP methods in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "e0376f07ae23fb13b34d659c968f0df7",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "import pprint\n",
    "\n",
    "from nltk import ne_chunk\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.cross_validation import check_random_state\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "d643d52a5785d3b8395ac08aa3b99fd1",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Basic Concepts\n",
    "\n",
    "Let's explore some basic concepts of part of speech (POS) tagging.\n",
    "\n",
    "We use *Alice's Adventures in Wonderland* by Lewis Carroll, freely available from *Project Gutenberg*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "e1d8f1a4c962f13d2ce72b6ef94d3b37",
     "grade": false,
     "grade_id": "download_text",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.org\r\n",
      "\r\n",
      "\r\n",
      "Title: Alice's Adventures in Wonderland\r\n",
      "\r\n",
      "Author: Lewis Carroll\r\n",
      "\r\n",
      "Posting Date: June 25, 2008 [EBook #11]\r\n",
      "Release Date: March, 1994\r\n",
      "[Last updated: December 20, 2011]\r\n",
      "\r\n",
      "Language: English\r\n",
      "\r\n",
      "\r\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "ALICE'S ADVENTURES IN WONDERLAND\r\n",
      "\r\n",
      "Lewis Carroll\r\n",
      "\r\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "CHAPTER I. Down the Rabbit-Hole\r\n",
      "\r\n",
      "Alice was beginning to get very tired of sitting by her sister on the\r\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\r\n",
      "book her sister was reading, but it had no pictures or conversations in\r\n",
      "it, 'and what is the u\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get('http://www.gutenberg.org/cache/epub/11/pg11.txt')\n",
    "text = resp.text\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "f092c8d895ac87adc2c8eb5bdec57d2b",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Tokenize\n",
    "\n",
    "- Function `tokenize()` calls `word_tokenize()` to tokenize the text by words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "1049f6b99f2ac5a8a621c71c9ae1e4dd",
     "grade": false,
     "grade_id": "tokenize_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36719 words in course description\n",
      "----------------------------------------\n",
      "['\\ufeffProject', 'Gutenberg', \"'s\", 'Alice', \"'s\", 'Adventures', 'in', 'Wonderland', ',', 'by', 'Lewis', 'Carroll', 'This']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = tokenize(text)\n",
    "print('{0} words in course description'.format(len(word_tokens)))\n",
    "print(40*'-')\n",
    "print(word_tokens[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "6b61b744bb36e83abd036642f3a06707",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Collocations\n",
    "\n",
    "- Function `find_best_bigrams()` builds bigram collocations by using the pointwise mutual information (PMI).\n",
    "- The 10 best bigrams are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "685fd50c690e6f7d4f02c8c01838c9b1",
     "grade": false,
     "grade_id": "find_best_bigrams_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 10 bi-grams in text (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('#', '11'),\n",
      "  (\"'Cheshire\", 'Puss'),\n",
      "  (\"'IT\", 'DOES'),\n",
      "  (\"'ORANGE\", 'MARMALADE'),\n",
      "  (\"'Ou\", 'est'),\n",
      "  (\"'Rule\", 'Forty-two'),\n",
      "  (\"'Seven\", 'jogged'),\n",
      "  (\"'With\", 'extras'),\n",
      "  (\"'any\", 'shrimp'),\n",
      "  (\"'than\", 'waste')]\n"
     ]
    }
   ],
   "source": [
    "top_bigrams = find_best_bigrams(word_tokens)\n",
    "\n",
    "print('Best {0} bi-grams in text (WP Tokenizer)'.format(10))\n",
    "print(50*'-')\n",
    "\n",
    "ppf = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=False)\n",
    "ppf.pprint(top_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "0850ae3cc87f05f708458fd4feef57cf",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### DefaultTagger\n",
    "\n",
    "- Function `tag_words()` uses `DefaultTagger` to associate a tag of our choosing (the `tag` parameter) with words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "842a8306e01c0bd24af2fd5c9c4adc41",
     "grade": false,
     "grade_id": "tag_words_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged text (WP Tokenizer)\n",
      "--------------------------------------------------\n",
      "[ ('\\ufeffProject', 'INFO'), ('Gutenberg', 'INFO'), (\"'s\", 'INFO'),\n",
      "  ('Alice', 'INFO'), (\"'s\", 'INFO'), ('Adventures', 'INFO'), ('in', 'INFO'),\n",
      "  ('Wonderland', 'INFO'), (',', 'INFO'), ('by', 'INFO'), ('Lewis', 'INFO'),\n",
      "  ('Carroll', 'INFO'), ('This', 'INFO'), ('eBook', 'INFO'), ('is', 'INFO')]\n"
     ]
    }
   ],
   "source": [
    "tags = tag_words(word_tokens, 'INFO')\n",
    "print('Tagged text (WP Tokenizer)')\n",
    "print(50*'-')\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "pp.pprint(tags[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "8ce24a22699a5108627910367e2a700b",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Part of Speech Tagging\n",
    "\n",
    "- Function `tag_pos()` uses a PerceptronTagger to create Part of Speech (PoS) tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "8f7ee272139a238d4a349174cb4073ec",
     "grade": false,
     "grade_id": "tag_pos_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoS tagged text (WP Tokenizer/Univesal Tagger)\n",
      "------------------------------------------------------------\n",
      "[ ('\\ufeffProject', 'JJ'),\n",
      "  ('Gutenberg', 'NNP'),\n",
      "  (\"'s\", 'POS'),\n",
      "  ('Alice', 'NNP'),\n",
      "  (\"'s\", 'POS'),\n",
      "  ('Adventures', 'NNS'),\n",
      "  ('in', 'IN'),\n",
      "  ('Wonderland', 'NNP'),\n",
      "  (',', ','),\n",
      "  ('by', 'IN'),\n",
      "  ('Lewis', 'NNP'),\n",
      "  ('Carroll', 'NNP'),\n",
      "  ('This', 'DT'),\n",
      "  ('eBook', 'NN'),\n",
      "  ('is', 'VBZ')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = tag_pos(word_tokens)\n",
    "\n",
    "print('PoS tagged text (WP Tokenizer/Univesal Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(pos_tags[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "3a255b65079798e9fcfebaade4299ce7",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Penn Treebank\n",
    "\n",
    "- Function `tag_penn()` tokenizes and tags unigrams in `text` by using `UnigramTagger` and a Penn Treebank tagged sentence and word tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "e6d7f7a37f917b3982fb13f3a00a881d",
     "grade": false,
     "grade_id": "tag_penn_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penn Treebank tagged text (WP Tokenizer)\n",
      "------------------------------------------------------------\n",
      "[ ('\\ufeffProject', None),\n",
      "  ('Gutenberg', None),\n",
      "  (\"'s\", 'POS'),\n",
      "  ('Alice', None),\n",
      "  (\"'s\", 'POS'),\n",
      "  ('Adventures', None),\n",
      "  ('in', 'IN'),\n",
      "  ('Wonderland', None),\n",
      "  (',', ','),\n",
      "  ('by', 'IN'),\n",
      "  ('Lewis', 'NNP'),\n",
      "  ('Carroll', None),\n",
      "  ('This', 'DT'),\n",
      "  ('eBook', None),\n",
      "  ('is', 'VBZ')]\n"
     ]
    }
   ],
   "source": [
    "b_tags = tag_penn(word_tokens)\n",
    "\n",
    "print('Penn Treebank tagged text (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(b_tags[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "7c2a6e55c35a8f1ea23f547555cb4302",
     "grade": false,
     "grade_id": "markdown_7",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Linking Taggers\n",
    "\n",
    "- Function `tag_linked()` links the Penn Treebank Corpus tagger with our earlier Default tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "041a0f981d12469610d6c13bd4e703f0",
     "grade": false,
     "grade_id": "tag_linked_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penn Treebank tagged text (WP Tokenizer/Linked Tagger)\n",
      "------------------------------------------------------------\n",
      "[ ('\\ufeffProject', 'INFO'),\n",
      "  ('Gutenberg', 'INFO'),\n",
      "  (\"'s\", 'POS'),\n",
      "  ('Alice', 'INFO'),\n",
      "  (\"'s\", 'POS'),\n",
      "  ('Adventures', 'INFO'),\n",
      "  ('in', 'IN'),\n",
      "  ('Wonderland', 'INFO'),\n",
      "  (',', ','),\n",
      "  ('by', 'IN'),\n",
      "  ('Lewis', 'NNP'),\n",
      "  ('Carroll', 'INFO'),\n",
      "  ('This', 'DT'),\n",
      "  ('eBook', 'INFO'),\n",
      "  ('is', 'VBZ')]\n"
     ]
    }
   ],
   "source": [
    "linked_tags = tag_linked(word_tokens)\n",
    "print('Penn Treebank tagged text (WP Tokenizer/Linked Tagger)')\n",
    "print(60*'-')\n",
    "\n",
    "ppf.pprint(linked_tags[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "cab64368fd013cf60da2e085fbd5432d",
     "grade": false,
     "grade_id": "markdown_8",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Tagged Text Extraction\n",
    "\n",
    "- We can use regular expressions to restrict tokens in the text to Nouns, Verbs, Adjectives, and Adverbs.\n",
    "- Function `extract_tags()` returns a tuple of PoS tags and the extracted terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "9aaa60ef8f629f13b4a319b36b4cf1a0",
     "grade": false,
     "grade_id": "extract_tags_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged text (WP Tokenizer)\n",
      "------------------------------------------------------------\n",
      "[ ('\\ufeffProject', 'JJ'), ('Gutenberg', 'NNP'), (\"'s\", 'POS'),\n",
      "  ('Alice', 'NNP'), (\"'s\", 'POS'), ('Adventures', 'NNS'), ('in', 'IN'),\n",
      "  ('Wonderland', 'NNP'), (',', ','), ('by', 'IN'), ('Lewis', 'NNP'),\n",
      "  ('Carroll', 'NNP'), ('This', 'DT'), ('eBook', 'NN'), ('is', 'VBZ')]\n",
      "------------------------------------------------------------\n",
      "POS tagged text (WP Tokenizer/RegEx applied)\n",
      "------------------------------------------------------------\n",
      "[ '\\ufeffProject', 'Gutenberg', 'Alice', 'Adventures', 'Wonderland', 'Lewis',\n",
      "  'Carroll', 'eBook', 'is', 'use', 'anyone', 'anywhere', 'cost', 'almost',\n",
      "  'restrictions']\n"
     ]
    }
   ],
   "source": [
    "pos_tags, terms = extract_tags(word_tokens)\n",
    "\n",
    "print('POS tagged text (WP Tokenizer)')\n",
    "print(60*'-')\n",
    "pp.pprint(pos_tags[:15])\n",
    "print(60*'-')\n",
    "print('POS tagged text (WP Tokenizer/RegEx applied)')\n",
    "print(60*'-')\n",
    "pp.pprint(terms[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "a630cc19372c4e42474625136a001138",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Let's move on to the concept of topic modeling. We use the twenty newsgroup data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "129bdcdf3b926ea610045a749bf95b13",
     "grade": false,
     "grade_id": "20newsgroups",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(\n",
    "    data_home='~/textdm', \n",
    "    subset='train',\n",
    "    shuffle=True,\n",
    "    random_state=check_random_state(0),\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    "    )\n",
    "\n",
    "test = fetch_20newsgroups(\n",
    "    data_home='~/textdm', \n",
    "    subset='test',\n",
    "    shuffle=True,\n",
    "    random_state=check_random_state(0),\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "9021cc7d91c1aba4875327bd7f61746a",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Document term matrix\n",
    "\n",
    "- We will use the function `get_document_term_matrix()`\n",
    "- Uses TfidfVectorizer to create a document term matrix for both `train['data']` and `test['data']`.\n",
    "- Uses English stop words.\n",
    "- Uses unigrams and bigrams.\n",
    "- Ignores terms that have a document frequency strictly lower than 2.\n",
    "- Builds a vocabulary that only consider the top 20,000 features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "223df1d07ede85ba51041e4d98fec8cf",
     "grade": false,
     "grade_id": "get_document_term_matrix_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "cv, train_data, test_data = get_document_term_matrix(train['data'], test['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "0c94a4f3e3244ae3a1e134a7913a332c",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Non-negative matrix factorization\n",
    "\n",
    "- We will use function `apply_nmf()`\n",
    "- Applies non-negative matrix factorization (NMF) to compute topics in `train_data`.\n",
    "- Uses 60 topics.\n",
    "- Normalizes the transformed data to have unit probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "bb8661b4ac2fa4a913f92dd86fff2d47",
     "grade": false,
     "grade_id": "apply_nmf_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120597</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>0.037898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005427</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.033312</td>\n",
       "      <td>0.019869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.os.ms-windows.misc</th>\n",
       "      <td>0.012951</td>\n",
       "      <td>0.207267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.057188</td>\n",
       "      <td>0.007003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.ibm.pc.hardware</th>\n",
       "      <td>0.253397</td>\n",
       "      <td>0.015348</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017412</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.mac.hardware</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.641944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.windows.x</th>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014806</td>\n",
       "      <td>0.367371</td>\n",
       "      <td>0.019263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misc.forsale</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.048850</td>\n",
       "      <td>0.579381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>0.009420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083633</td>\n",
       "      <td>0.001053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.autos</th>\n",
       "      <td>0.044051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.motorcycles</th>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104846</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.baseball</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042552</td>\n",
       "      <td>0.012623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <td>0.007178</td>\n",
       "      <td>0.057966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044383</td>\n",
       "      <td>0.020401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028229</td>\n",
       "      <td>0.038843</td>\n",
       "      <td>0.032829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017458</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.crypt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059774</td>\n",
       "      <td>0.022716</td>\n",
       "      <td>0.011545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168861</td>\n",
       "      <td>0.018223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.141698</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.electronics</th>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>0.035532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134462</td>\n",
       "      <td>0.033779</td>\n",
       "      <td>0.012454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>0.288122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.med</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>0.026780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049695</td>\n",
       "      <td>0.032267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <td>0.008090</td>\n",
       "      <td>0.010245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006528</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012259</td>\n",
       "      <td>0.018644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055271</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.guns</th>\n",
       "      <td>0.135704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061471</td>\n",
       "      <td>0.334120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038932</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090331</td>\n",
       "      <td>0.325750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.misc</th>\n",
       "      <td>0.027812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004852</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283583</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.198373</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0         1         2         3         4   \\\n",
       "label                                                                        \n",
       "alt.atheism               0.000000  0.004216  0.000000  0.120597  0.001416   \n",
       "comp.graphics             0.037898  0.000000  0.000000  0.209269  0.000000   \n",
       "comp.os.ms-windows.misc   0.012951  0.207267  0.000000  0.000000  0.000000   \n",
       "comp.sys.ibm.pc.hardware  0.253397  0.015348  0.003114  0.000000  0.241721   \n",
       "comp.sys.mac.hardware     0.000000  0.000000  0.000000  0.641944  0.000000   \n",
       "comp.windows.x            0.002888  0.004130  0.000000  0.000000  0.048298   \n",
       "misc.forsale              0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "rec.autos                 0.044051  0.000000  0.000000  0.000000  0.149592   \n",
       "rec.motorcycles           0.000880  0.000000  0.004938  0.002977  0.001808   \n",
       "rec.sport.baseball        0.000000  0.000000  0.000000  0.004005  0.000000   \n",
       "rec.sport.hockey          0.007178  0.057966  0.000000  0.000000  0.000000   \n",
       "sci.crypt                 0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "sci.electronics           0.000492  0.000000  0.000000  0.000000  0.006378   \n",
       "sci.med                   0.000000  0.000000  0.000900  0.000000  0.000000   \n",
       "sci.space                 0.026780  0.000000  0.000000  0.000000  0.000000   \n",
       "soc.religion.christian    0.008090  0.010245  0.000000  0.006528  0.005192   \n",
       "talk.politics.guns        0.135704  0.000000  0.217073  0.000000  0.000000   \n",
       "talk.politics.mideast     0.000000  0.000000  0.000000  0.038932  0.005279   \n",
       "talk.politics.misc        0.027812  0.000000  0.000000  0.004852  0.003961   \n",
       "talk.religion.misc        0.000000  0.000000  0.000000  0.009120  0.000000   \n",
       "\n",
       "                                5         6         7         8         9   \\\n",
       "label                                                                        \n",
       "alt.atheism               0.010014  0.000491  0.000000  0.000000  0.000000   \n",
       "comp.graphics             0.000000  0.000000  0.003670  0.000000  0.012231   \n",
       "comp.os.ms-windows.misc   0.000000  0.000000  0.002893  0.057188  0.007003   \n",
       "comp.sys.ibm.pc.hardware  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "comp.sys.mac.hardware     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "comp.windows.x            0.000000  0.000000  0.000000  0.006685  0.000000   \n",
       "misc.forsale              0.000000  0.086419  0.000000  0.007352  0.000000   \n",
       "rec.autos                 0.000000  0.000000  0.030260  0.000000  0.000000   \n",
       "rec.motorcycles           0.000000  0.003660  0.000000  0.000000  0.000000   \n",
       "rec.sport.baseball        0.000000  0.009396  0.000000  0.042552  0.012623   \n",
       "rec.sport.hockey          0.000000  0.000000  0.000000  0.044383  0.020401   \n",
       "sci.crypt                 0.000000  0.059774  0.022716  0.011545  0.000000   \n",
       "sci.electronics           0.000000  0.007209  0.035532  0.000000  0.000000   \n",
       "sci.med                   0.071784  0.000000  0.000000  0.000000  0.000000   \n",
       "sci.space                 0.000000  0.000000  0.000000  0.000000  0.534881   \n",
       "soc.religion.christian    0.000000  0.000000  0.000000  0.012259  0.018644   \n",
       "talk.politics.guns        0.061471  0.334120  0.000000  0.000000  0.000000   \n",
       "talk.politics.mideast     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "talk.politics.misc        0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "talk.religion.misc        0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                            ...           50        51        52        53  \\\n",
       "label                       ...                                              \n",
       "alt.atheism                 ...     0.000000  0.000000  0.001455  0.000000   \n",
       "comp.graphics               ...     0.000000  0.000000  0.000000  0.000000   \n",
       "comp.os.ms-windows.misc     ...     0.000000  0.015903  0.000000  0.000000   \n",
       "comp.sys.ibm.pc.hardware    ...     0.000000  0.000000  0.109778  0.000000   \n",
       "comp.sys.mac.hardware       ...     0.000000  0.000000  0.000000  0.000000   \n",
       "comp.windows.x              ...     0.003866  0.000000  0.000000  0.002701   \n",
       "misc.forsale                ...     0.005670  0.048850  0.579381  0.000000   \n",
       "rec.autos                   ...     0.000000  0.000000  0.000000  0.000000   \n",
       "rec.motorcycles             ...     0.000000  0.000000  0.067102  0.000000   \n",
       "rec.sport.baseball          ...     0.000000  0.000000  0.000000  0.000000   \n",
       "rec.sport.hockey            ...     0.028229  0.038843  0.032829  0.000000   \n",
       "sci.crypt                   ...     0.000000  0.168861  0.018223  0.000000   \n",
       "sci.electronics             ...     0.000000  0.134462  0.033779  0.012454   \n",
       "sci.med                     ...     0.000000  0.000000  0.000000  0.000000   \n",
       "sci.space                   ...     0.001837  0.000000  0.000000  0.000000   \n",
       "soc.religion.christian      ...     0.000000  0.000000  0.078329  0.000000   \n",
       "talk.politics.guns          ...     0.000000  0.000000  0.000000  0.000000   \n",
       "talk.politics.mideast       ...     0.000000  0.000000  0.000000  0.000000   \n",
       "talk.politics.misc          ...     0.000000  0.000000  0.022432  0.000000   \n",
       "talk.religion.misc          ...     0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                                54        55        56        57        58  \\\n",
       "label                                                                        \n",
       "alt.atheism               0.000000  0.000000  0.434272  0.000000  0.003139   \n",
       "comp.graphics             0.034581  0.000000  0.005427  0.003008  0.033312   \n",
       "comp.os.ms-windows.misc   0.000000  0.000000  0.000000  0.059286  0.000000   \n",
       "comp.sys.ibm.pc.hardware  0.000000  0.034120  0.000000  0.000000  0.017412   \n",
       "comp.sys.mac.hardware     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "comp.windows.x            0.000000  0.013505  0.000000  0.014806  0.367371   \n",
       "misc.forsale              0.000000  0.007527  0.009420  0.000000  0.083633   \n",
       "rec.autos                 0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "rec.motorcycles           0.000000  0.000000  0.002896  0.000000  0.104846   \n",
       "rec.sport.baseball        0.000000  0.675968  0.000000  0.000000  0.000000   \n",
       "rec.sport.hockey          0.000000  0.000000  0.017458  0.002639  0.000000   \n",
       "sci.crypt                 0.001284  0.000000  0.000000  0.002301  0.141698   \n",
       "sci.electronics           0.000000  0.000000  0.000000  0.000000  0.007737   \n",
       "sci.med                   0.000000  0.000000  0.000000  0.000000  0.012459   \n",
       "sci.space                 0.000000  0.003723  0.000000  0.000000  0.049695   \n",
       "soc.religion.christian    0.000000  0.000000  0.000000  0.000000  0.055271   \n",
       "talk.politics.guns        0.000000  0.000000  0.000000  0.000000  0.023017   \n",
       "talk.politics.mideast     0.000000  0.000000  0.000000  0.090331  0.325750   \n",
       "talk.politics.misc        0.018021  0.000000  0.009741  0.000000  0.283583   \n",
       "talk.religion.misc        0.000000  0.000490  0.000000  0.001880  0.198373   \n",
       "\n",
       "                                59  \n",
       "label                               \n",
       "alt.atheism               0.000000  \n",
       "comp.graphics             0.019869  \n",
       "comp.os.ms-windows.misc   0.000000  \n",
       "comp.sys.ibm.pc.hardware  0.000000  \n",
       "comp.sys.mac.hardware     0.000000  \n",
       "comp.windows.x            0.019263  \n",
       "misc.forsale              0.001053  \n",
       "rec.autos                 0.000000  \n",
       "rec.motorcycles           0.000000  \n",
       "rec.sport.baseball        0.001574  \n",
       "rec.sport.hockey          0.000000  \n",
       "sci.crypt                 0.000000  \n",
       "sci.electronics           0.288122  \n",
       "sci.med                   0.000000  \n",
       "sci.space                 0.032267  \n",
       "soc.religion.christian    0.000000  \n",
       "talk.politics.guns        0.000000  \n",
       "talk.politics.mideast     0.000000  \n",
       "talk.politics.misc        0.000000  \n",
       "talk.religion.misc        0.000000  \n",
       "\n",
       "[20 rows x 60 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf, td_norm = apply_nmf(train_data, random_state=check_random_state(0))\n",
    "\n",
    "# We use a DataFrame to simplify the collecting of the data for display.\n",
    "df = pd.DataFrame(td_norm)\n",
    "df.fillna(value=0, inplace=True)\n",
    "df['label'] = pd.Series(train['target_names'], dtype=\"category\")\n",
    "\n",
    "df.groupby('label').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "23c2e6fe8c633a0f8a8d7d40fd8faa4a",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Topic-based Classification\n",
    "\n",
    "- Let's train a Random Forest classifier on the topics in the training data sample of the twenty newsgroup data set.\n",
    "- Function `classify_topics()` compute the topics, by using the previously created NMF model, for the test data and compute classifications from these topic models. \n",
    "\n",
    "The resulting classification report and confusion matrix are shown to demonstrate the quality of this classification method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "c99f938210b1f9448cbbc735244c7d28",
     "grade": false,
     "grade_id": "classify_topics_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.26      0.34      0.29       319\n",
      "           comp.graphics       0.38      0.52      0.44       389\n",
      " comp.os.ms-windows.misc       0.45      0.47      0.46       394\n",
      "comp.sys.ibm.pc.hardware       0.47      0.49      0.48       392\n",
      "   comp.sys.mac.hardware       0.52      0.51      0.52       385\n",
      "          comp.windows.x       0.62      0.57      0.59       395\n",
      "            misc.forsale       0.73      0.69      0.71       390\n",
      "               rec.autos       0.42      0.66      0.52       396\n",
      "         rec.motorcycles       0.66      0.62      0.64       398\n",
      "      rec.sport.baseball       0.52      0.52      0.52       397\n",
      "        rec.sport.hockey       0.65      0.61      0.63       399\n",
      "               sci.crypt       0.66      0.59      0.62       396\n",
      "         sci.electronics       0.38      0.31      0.34       393\n",
      "                 sci.med       0.61      0.56      0.58       396\n",
      "               sci.space       0.63      0.60      0.62       394\n",
      "  soc.religion.christian       0.54      0.66      0.59       398\n",
      "      talk.politics.guns       0.50      0.51      0.51       364\n",
      "   talk.politics.mideast       0.80      0.67      0.73       376\n",
      "      talk.politics.misc       0.27      0.19      0.22       310\n",
      "      talk.religion.misc       0.16      0.06      0.08       251\n",
      "\n",
      "             avg / total       0.52      0.52      0.52      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf, ts_preds = classify_topics(\n",
    "    nmf, nmf.transform(train_data), train['target'], test_data, check_random_state(0)\n",
    "    )\n",
    "print(classification_report(test['target'], ts_preds, target_names=test['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "7f851ecfea4ac3fe4befe936948cb928",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Topic Modeling with Gensim\n",
    "\n",
    "- We could also use the gensim library to perform topic modeling of the twenty newsgroup data. First we transform a sparse matrix into a gensim corpus, and then construct a vocabulary dictionary. Finally, we create a  Latent Dirichlet allocation (LDA) model with 20 topics for the newsgroup text, and return 5 most significant words for each topic.\n",
    "- We specify three parameters in `LdaModel()`: `corpus`, `id2word`, and `num_topics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "a59f5f881e6e96f699813e444c0ba77d",
     "grade": false,
     "grade_id": "get_topics_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "-----------------------------------\n",
      "    pitt                : 0.0054\n",
      "    banks               : 0.0049\n",
      "    pitt edu            : 0.0046\n",
      "    shameful            : 0.0044\n",
      "    intellect           : 0.0043\n",
      "-----------------------------------\n",
      "Topic 1\n",
      "-----------------------------------\n",
      "    people              : 0.0029\n",
      "    just                : 0.0029\n",
      "    don                 : 0.0029\n",
      "    like                : 0.0028\n",
      "    know                : 0.0027\n",
      "-----------------------------------\n",
      "Topic 2\n",
      "-----------------------------------\n",
      "    god                 : 0.0076\n",
      "    people              : 0.0045\n",
      "    don                 : 0.0036\n",
      "    think               : 0.0035\n",
      "    believe             : 0.0034\n",
      "-----------------------------------\n",
      "Topic 3\n",
      "-----------------------------------\n",
      "    zip                 : 0.0027\n",
      "    help                : 0.0022\n",
      "    thanks              : 0.0020\n",
      "    like                : 0.0019\n",
      "    use                 : 0.0018\n",
      "-----------------------------------\n",
      "Topic 4\n",
      "-----------------------------------\n",
      "    card                : 0.0043\n",
      "    software            : 0.0042\n",
      "    monitor             : 0.0037\n",
      "    windows             : 0.0036\n",
      "    drive               : 0.0035\n",
      "-----------------------------------\n",
      "Topic 5\n",
      "-----------------------------------\n",
      "    thanks              : 0.0034\n",
      "    scsi                : 0.0032\n",
      "    know                : 0.0028\n",
      "    apple               : 0.0027\n",
      "    does                : 0.0025\n",
      "-----------------------------------\n",
      "Topic 6\n",
      "-----------------------------------\n",
      "    key                 : 0.0100\n",
      "    chip                : 0.0055\n",
      "    keys                : 0.0053\n",
      "    clipper             : 0.0051\n",
      "    president           : 0.0042\n",
      "-----------------------------------\n",
      "Topic 7\n",
      "-----------------------------------\n",
      "    just                : 0.0038\n",
      "    game                : 0.0036\n",
      "    team                : 0.0031\n",
      "    thanks              : 0.0029\n",
      "    games               : 0.0029\n",
      "-----------------------------------\n",
      "Topic 8\n",
      "-----------------------------------\n",
      "    ama                 : 0.0031\n",
      "    blah                : 0.0029\n",
      "    slick               : 0.0027\n",
      "    complaint           : 0.0026\n",
      "    motorcycle          : 0.0026\n",
      "-----------------------------------\n",
      "Topic 9\n",
      "-----------------------------------\n",
      "    mailing list        : 0.0033\n",
      "    tyre                : 0.0027\n",
      "    mailing             : 0.0027\n",
      "    po                  : 0.0026\n",
      "    ieee                : 0.0024\n",
      "-----------------------------------\n",
      "Topic 10\n",
      "-----------------------------------\n",
      "    msg                 : 0.0027\n",
      "    just                : 0.0027\n",
      "    like                : 0.0025\n",
      "    water               : 0.0025\n",
      "    food                : 0.0021\n",
      "-----------------------------------\n",
      "Topic 11\n",
      "-----------------------------------\n",
      "    cview               : 0.0042\n",
      "    holocaust           : 0.0024\n",
      "    instruction         : 0.0023\n",
      "    mileage             : 0.0022\n",
      "    tiny                : 0.0022\n",
      "-----------------------------------\n",
      "Topic 12\n",
      "-----------------------------------\n",
      "    encryption          : 0.0040\n",
      "    escrow              : 0.0030\n",
      "    homosexual          : 0.0024\n",
      "    sex                 : 0.0024\n",
      "    government          : 0.0024\n",
      "-----------------------------------\n",
      "Topic 13\n",
      "-----------------------------------\n",
      "    login               : 0.0031\n",
      "    dial                : 0.0029\n",
      "    graduate            : 0.0027\n",
      "    dale                : 0.0026\n",
      "    disc                : 0.0025\n",
      "-----------------------------------\n",
      "Topic 14\n",
      "-----------------------------------\n",
      "    __                  : 0.0038\n",
      "    cheers kent         : 0.0031\n",
      "    cheers              : 0.0028\n",
      "    kent                : 0.0027\n",
      "    aluminum            : 0.0026\n",
      "-----------------------------------\n",
      "Topic 15\n",
      "-----------------------------------\n",
      "    dog                 : 0.0038\n",
      "    norton              : 0.0022\n",
      "    esdi                : 0.0019\n",
      "    bikers              : 0.0019\n",
      "    mom                 : 0.0019\n",
      "-----------------------------------\n",
      "Topic 16\n",
      "-----------------------------------\n",
      "    deleted             : 0.0041\n",
      "    pitching            : 0.0040\n",
      "    test                : 0.0039\n",
      "    stuff deleted       : 0.0034\n",
      "    cycle               : 0.0033\n",
      "-----------------------------------\n",
      "Topic 17\n",
      "-----------------------------------\n",
      "    armenian            : 0.0037\n",
      "    annoying            : 0.0033\n",
      "    captain             : 0.0032\n",
      "    icon                : 0.0030\n",
      "    cancer              : 0.0030\n",
      "-----------------------------------\n",
      "Topic 18\n",
      "-----------------------------------\n",
      "    mouse               : 0.0032\n",
      "    amendment           : 0.0028\n",
      "    sigh                : 0.0028\n",
      "    militia             : 0.0027\n",
      "    bat                 : 0.0027\n",
      "-----------------------------------\n",
      "Topic 19\n",
      "-----------------------------------\n",
      "    adam                : 0.0039\n",
      "    freedom             : 0.0029\n",
      "    ini                 : 0.0027\n",
      "    just wondering      : 0.0026\n",
      "    huh                 : 0.0025\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics(cv, train_data)\n",
    "\n",
    "for idx, (lst, val) in enumerate(topics):\n",
    "    print('Topic {0}'.format(idx))\n",
    "    print(35*('-'))\n",
    "    for i, z in lst:\n",
    "        print('    {0:20s}: {1:5.4f}'.format(z, i))\n",
    "    print(35*('-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "0ab5720999b76fb005d451a1d1b8e498",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Semantic Analysis\n",
    "\n",
    "Let's move on to semantic analysis.\n",
    "\n",
    "### Wordnet\n",
    "\n",
    "We use the Wordnet synonym rings.\n",
    "\n",
    "- Function `find_number_of_entries_in_synonym_ring()` finds how many entries a word has in the wordnet synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "9047c17d4e6a474ad25ac66eb5ea83c6",
     "grade": false,
     "grade_id": "find_number_of_entries_in_synonym_ring_run1",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 total entries in synonym ring for ship. \n"
     ]
    }
   ],
   "source": [
    "the_word = 'ship'\n",
    "n_entries = find_number_of_entries_in_synonym_ring(the_word)\n",
    "print('{0} total entries in synonym ring for {1}. '.format(n_entries, the_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "8146c0b5ba56e5ccaf72cecbe5237fbb",
     "grade": false,
     "grade_id": "find_number_of_entries_in_synonym_ring_run2",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 total entries in synonym ring for throw. \n"
     ]
    }
   ],
   "source": [
    "the_word = 'throw'\n",
    "n_entries = find_number_of_entries_in_synonym_ring(the_word)\n",
    "print('{0} total entries in synonym ring for {1}. '.format(n_entries, the_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "d1b714b9ec3da0894b8208fc3af5f06a",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Word Similarities\n",
    "\n",
    "- Here we have four functions that will compute word similarities.\n",
    "- Computes the path similarity for _cat_, _dog_, _boy_, and _girl_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "2fe010b2c7b1590633fef782321a4de5",
     "grade": false,
     "grade_id": "get_path_similarity_between_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path Similarity:\n",
      "----------------------------------------\n",
      "boy to girl: 0.167\n",
      "boy to cat: 0.083\n",
      "boy to dog: 0.143\n",
      "girl to girl: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Now we print similarity measures.\n",
    "fmt_str = '{1} to {2}: {0:4.3f}'\n",
    "\n",
    "print('Path Similarity:')\n",
    "print(40*'-')\n",
    "print(fmt_str.format(get_path_similarity_between_boy_and_girl(), 'boy', 'girl'))\n",
    "print(fmt_str.format(get_path_similarity_between_boy_and_cat(), 'boy', 'cat'))\n",
    "print(fmt_str.format(get_path_similarity_between_boy_and_dog(), 'boy', 'dog'))\n",
    "print(fmt_str.format(get_path_similarity_between_girl_and_girl(), 'girl', 'girl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "3a220080885be128eb8898f71aa51458",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Word2Vec\n",
    "\n",
    "Let's use the NLTK Brown corpus to build a word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "78efef9ac888ebce3fb972ac90a7e589",
     "grade": false,
     "grade_id": "brown",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "sentences = brown.sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "8fe1326710e24021269e031a7fcd33e1",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Word2Vec model\n",
    "\n",
    "- Our function `get_model()` will handle the Word2Vec model\n",
    "- Builds model from movie reviews `new_mvr`.\n",
    "- The maximum distance between the current and predicted word within a sentence is set to 10.\n",
    "- Ignores all words with total frequency lower than 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "The following code cell takes a while to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "nbgrader": {
     "checksum": "85c5c764ca95e964dc60104696b67b21",
     "grade": false,
     "grade_id": "get_model_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "model = get_model(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "aa1a1846d22c61f9448fda90b562751b",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "- Function `get_cosine_similarity()` computes Cosine Similarities.\n",
    "- Cosine Similarity measures the similarity between two vectors by computing the cosine of the angle between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "e1ef6e6d34054b9d108ae029a5b89ff9",
     "grade": false,
     "grade_id": "get_cosine_similarity_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity:\n",
      "----------------------------------------\n",
      "boy to girl: 0.947\n",
      "boy to cat: 0.576\n",
      "boy to dog: 0.716\n",
      "girl to girl: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Now we print similarity measures.\n",
    "fmt_str = '{1} to {2}: {0:4.3f}'\n",
    "\n",
    "print('Cosine Similarity:')\n",
    "print(40*'-')\n",
    "print(fmt_str.format(get_cosine_similarity(model, 'boy', 'girl'), 'boy', 'girl'))\n",
    "print(fmt_str.format(get_cosine_similarity(model, 'boy', 'cat'), 'boy', 'cat'))\n",
    "print(fmt_str.format(get_cosine_similarity(model, 'boy', 'dog'), 'boy', 'dog'))\n",
    "print(fmt_str.format(get_cosine_similarity(model, 'girl', 'girl'), 'girl', 'girl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "nbgrader": {
     "checksum": "017417df44e1e49235479760d5ca45c8",
     "grade": false,
     "grade_id": "markdown_7",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Most similar words\n",
    "\n",
    "- We create a function called `find_most_similar_words()`\n",
    "- Finds the top 3 most similar words, where \"girl\" and \"cat\" contribute positively towards the similarity, and \"boy\" and \"dog\" contribute negatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "nbgrader": {
     "checksum": "6aab3fc1edcdbbcd3929b93ed244788d",
     "grade": false,
     "grade_id": "find_most_similar_words_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word          : Cosine Similarity\n",
      "----------------------------------------\n",
      "Joan's        :  0.661\n",
      "burglary      :  0.618\n",
      "correspondingly:  0.617\n"
     ]
    }
   ],
   "source": [
    "print('{0:14s}: {1}'.format('Word', 'Cosine Similarity'))\n",
    "print(40*'-')\n",
    "for val in find_most_similar_words(model):\n",
    "    print('{0:14s}: {1:6.3f}'.format(val[0], val[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
